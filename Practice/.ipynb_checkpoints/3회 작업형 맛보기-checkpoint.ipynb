{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d456d680",
   "metadata": {},
   "source": [
    "# 작업형1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c67cfb",
   "metadata": {},
   "source": [
    "#### 1. economics 데이터셋을 불러와 첫 번째 행부터 순서대로 70%까지의 데이터를 훈련 데이터로 추출한 후 pce 컬럼의 제 1사분위수를 구하라. (단, 정수로 나타낼 것)¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2771b24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1075\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "economics = pd.read_csv('data/기출복원/03회/economics.csv')\n",
    "per_70 = int(economics.shape[0]*0.7)\n",
    "\n",
    "train = economics[:per_70]\n",
    "\n",
    "result1 = train['pce'].quantile(q = 0.25).astype('int')\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18cc5e",
   "metadata": {},
   "source": [
    "#### 2. Hitters 데이터셋을 불러와 Years 컬럼이 10인 데이터만 추출하여 HmRun 컬럼이 평균보다 큰 선수가 몇 명인지 계산하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e2ab7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hitters = pd.read_csv('data/기출복원/03회/Hitters.csv')\n",
    "\n",
    "years_10 = hitters[hitters['Years'] == 10]\n",
    "mean_hr = years_10['HmRun'].mean()\n",
    "\n",
    "hr_over_10 = years_10[years_10['HmRun'] > mean_hr]\n",
    "\n",
    "result2 = len(hr_over_10)\n",
    "print(result2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f32b245",
   "metadata": {},
   "source": [
    "#### 3. msleep 데이터셋을 불러와 가장 결측치가 많은 컬럼의 이름을 출력하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1eb3dfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleep_cycle\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "msleep = pd.read_csv('data/기출복원/03회/msleep.csv')\n",
    "null_sleep = msleep.isnull().sum()\n",
    "\n",
    "max_null = null_sleep.max()\n",
    "\n",
    "idx = 0\n",
    "for i in null_sleep :\n",
    "    if i == max_null :\n",
    "        break\n",
    "    else :\n",
    "        idx += 1\n",
    "\n",
    "print(null_sleep.index[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b256fa2a",
   "metadata": {},
   "source": [
    "# 작업형2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc07088",
   "metadata": {},
   "source": [
    "#### 1. 아래는 HR 연구를 위한 이직 희망 여부와 입사 지원자들의 정보와 관련한 데이터의 일부이다.주어진 데이터를 이용하여 예측 모형을 만들고 아래에 따라 CSV 파일을 생성하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55b19f39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:05:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7521793353305929\n",
      "0.7336180589423814\n",
      "0.7255786246777678\n"
     ]
    }
   ],
   "source": [
    "### 데이터 불러오기\n",
    "import pandas as pd\n",
    "\n",
    "X_train = pd.read_csv('data/기출복원/03회/job_change_X_train.csv')\n",
    "X_test = pd.read_csv('data/기출복원/03회/job_change_X_test.csv')\n",
    "y_train = pd.read_csv('data/기출복원/03회/job_change_y_train.csv')\n",
    "\n",
    "### EDA\n",
    "\n",
    "    # 불필요 변수 'encollee_id'\n",
    "    # 범주형 ['city','gender','relevent_experience','enrolled_university','education_level','major_discipline','experience','company_size','company_type','last_new_job']\n",
    "    # 결측치 많음 'company_size','company_type'\n",
    "\n",
    "### 데이터 전처리\n",
    "# 1) 불필요 변수 'encollee_id' 제거\n",
    "X_train = X_train.drop('enrollee_id', axis = 1)\n",
    "y_train = y_train.drop('enrollee_id', axis = 1)\n",
    "test_id = X_test.pop('enrollee_id')\n",
    "\n",
    "# 2) 결측치 처리\n",
    "# 1000 이상 ['major_discipline','company_size','company_type']\n",
    "# -> 변수 제거\n",
    "X_train = X_train.drop(['major_discipline','company_size','company_type'], axis = 1)\n",
    "X_test = X_test.drop(['major_discipline','company_size','company_type'], axis = 1)\n",
    "\n",
    "# 200 미만 ['enrolled_university','education_level']\n",
    "# -> 최다 빈도로 대체\n",
    "max_univ = X_train['enrolled_university'].value_counts().idxmax()\n",
    "max_edu = X_train['education_level'].value_counts().idxmax()\n",
    "\n",
    "X_train['enrolled_university'].fillna(max_univ, inplace = True)\n",
    "X_train['education_level'].fillna(max_edu, inplace = True)\n",
    "\n",
    "X_test['enrolled_university'].fillna(max_univ, inplace = True)\n",
    "X_test['education_level'].fillna(max_edu, inplace = True)\n",
    "\n",
    "# 3) 범주형 변수 전처리\n",
    "# 4) 수치형 변수 전처리\n",
    "\n",
    "# 5) 데이터 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                  test_size = 0.2,\n",
    "                                                  random_state = 2022,\n",
    "                                                  stratify = y_train)\n",
    "# 6) 인코딩\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X_train_obj = X_train.select_dtypes('object').copy()\n",
    "X_val_obj = X_val.select_dtypes('object').copy()\n",
    "X_test_obj = X_test.select_dtypes('object').copy()\n",
    "\n",
    "ohe = OneHotEncoder(sparse = False, handle_unknown = 'ignore').fit(X_train_obj)\n",
    "\n",
    "X_train_oh = ohe.transform(X_train_obj)\n",
    "X_val_oh = ohe.transform(X_val_obj)\n",
    "X_test_oh = ohe.transform(X_test_obj)\n",
    "\n",
    "# 7) 스케일링\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train_num = X_train.select_dtypes(exclude = 'object').copy()\n",
    "X_val_num = X_val.select_dtypes(exclude = 'object').copy()\n",
    "X_test_num = X_test.select_dtypes(exclude = 'object').copy()\n",
    "\n",
    "scale = StandardScaler().fit(X_train_num)\n",
    "\n",
    "X_train_scale = scale.transform(X_train_num)\n",
    "X_val_scale = scale.transform(X_val_num)\n",
    "X_test_scale = scale.transform(X_test_num)\n",
    "\n",
    "# 8) 데이터셋 준비\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.concatenate([X_train_oh, X_train_scale], axis = 1)\n",
    "X_val = np.concatenate([X_val_oh, X_val_scale], axis = 1)\n",
    "X_test = np.concatenate([X_test_oh, X_test_scale], axis = 1)\n",
    "\n",
    "y_train = y_train.values.flatten()\n",
    "y_val = y_val.values.flatten()\n",
    "\n",
    "\n",
    "### 모델 학습\n",
    "# 학습\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 500,\n",
    "                            max_depth = 5,\n",
    "                            min_samples_leaf = 10,\n",
    "                            max_features = 'sqrt',\n",
    "                            random_state = 2022)\n",
    "model_rf = rf.fit(X_train, y_train)\n",
    "\n",
    "xgb = XGBClassifier(n_estimators = 500,\n",
    "                    max_depth = 8,\n",
    "                    nthread = 5,\n",
    "                    min_child_weight = 10,\n",
    "                    gamma = 0.5,\n",
    "                    objective = 'binary:logistic',\n",
    "                    use_label_encoder = False,\n",
    "                    random_state = 2022)\n",
    "model_xgb = xgb.fit(X_train, y_train)\n",
    "\n",
    "lgb = LGBMClassifier(n_estimators = 500,\n",
    "                     max_depth = 8,\n",
    "                     n_jobs = 5,\n",
    "                     min_child_weight = 10,\n",
    "                     learning_rate = 0.1,\n",
    "                     objective = 'binary',\n",
    "                     random_state = 2022)\n",
    "model_lgb = lgb.fit(X_train, y_train)\n",
    "\n",
    "# 검증\n",
    "score_rf = model_rf.predict_proba(X_val)[:,1]\n",
    "score_xgb = model_xgb.predict_proba(X_val)[:,1]\n",
    "score_lgb = model_lgb.predict_proba(X_val)[:,1]\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_val, score_rf)\n",
    "auc_rf = auc(fpr, tpr)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_val, score_xgb)\n",
    "auc_xgb = auc(fpr, tpr)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_val, score_lgb)\n",
    "auc_lgb = auc(fpr, tpr)\n",
    "\n",
    "print(auc_rf)\n",
    "print(auc_xgb)\n",
    "print(auc_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee9d2470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8b88b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
