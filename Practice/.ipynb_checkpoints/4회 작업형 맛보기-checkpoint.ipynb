{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ff3f45",
   "metadata": {},
   "source": [
    "# 작업형1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0959700",
   "metadata": {},
   "source": [
    "#### 1. Cars93 데이터셋을 불러와 Weight 컬럼에 대하여 아래의 과정을 수행하여라.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edaad7d",
   "metadata": {},
   "source": [
    "(가) 제 1사분위수와 제 2사분위수를 구하기 (나) 두 개의 차이의 절댓값을 구하기 (다) 그 값의 소수점을 버리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "502a0c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Cars93 = pd.read_csv('data/기출복원/04회/Cars93.csv')\n",
    "\n",
    "q1 = Cars93['Weight'].quantile(q = 0.25)\n",
    "q2 = Cars93['Weight'].quantile(q = 0.5)\n",
    "\n",
    "diff = abs(q1 - q2)\n",
    "\n",
    "result = int(round(diff, 0))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047eb6a1",
   "metadata": {},
   "source": [
    "#### 2. facebook_subset 데이터셋을 불러와 좋아요를 받은 전체 수 중 모바일에서 좋아요를 받은 비율을 구하고 그 비율이 0.6보다 크고 0.7보다 작으면서 성별이 남자인 경우의 레코드 수를 구하여라. (여기서, mobile_like_recived와 www_like_recived는 각각 모바일과 웹에서 좋아요를 받은 횟수이고 두 컬럼의 합은 좋아요를 받은 전체 수임)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "923d5872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "face = pd.read_csv('data/기출복원/04회/facebook_subset.csv')\n",
    "\n",
    "mobile = face['mobile_likes_received'].copy()\n",
    "www = face['www_likes_received'].copy()\n",
    "\n",
    "face['rate'] = mobile / (mobile+www)\n",
    "\n",
    "data = face[(face['rate'] > 0.6) & (face['rate'] < 0.7)]\n",
    "data_male = data[data['gender'] == 'male']\n",
    "\n",
    "result = len(data_male)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301d24e",
   "metadata": {},
   "source": [
    "#### 3. netflix_subset 데이터셋을 불러와 2021년 1월에 등록되었으면서 listed_in이 오직 Drama인 경우의 레코드 수를 구하여라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e9706e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "netflix = pd.read_csv('data/기출복원/04회/netflix_subset.csv', encoding = 'cp949')\n",
    "# 'date_added' 25-Sep-21\n",
    "# 'release_year' 2020\n",
    "# 'listed_in' 문자\n",
    "\n",
    "netflix['date_added'] = netflix['date_added'].str.lstrip()\n",
    "\n",
    "month = netflix['date_added'].str[3:6]\n",
    "year = netflix['date_added'].str[-2:]\n",
    "\n",
    "result = netflix[(year == '21') & (month == 'Jan') & (netflix['listed_in'] == 'Dramas')].shape[0]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdb69d4",
   "metadata": {},
   "source": [
    "# 작업형1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67c34ce",
   "metadata": {},
   "source": [
    "#### 1. 아래는 연령별 수행등급을 확인한 자료와 일부 운동 수행도 데이터이다. 주어진 데이터를 이용하여 예측 모형을 만들고 아래에 따라 CSV 파일을 생성하시오. (단, 제출 전 두 개 이상의 모형의 성능을 비교하여 가장 우수한 모형을 선정할 것)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "56363c53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The label must consist of integer labels of form 0, 1, 2, ..., [num_class - 1].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5592\\1695560198.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     92\u001b[0m                     \u001b[0muse_label_encoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                     random_state = 2022)\n\u001b[1;32m---> 94\u001b[1;33m \u001b[0mmodel_xgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_metric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'mlogloss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m lgb = LGBMClassifier(n_estimators = 300,\n",
      "\u001b[1;32m~\\.conda\\envs\\big_ana\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\big_ana\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1197\u001b[0m                 \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m             ):\n\u001b[1;32m-> 1199\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_encoding_check_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_xgb_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The label must consist of integer labels of form 0, 1, 2, ..., [num_class - 1]."
     ]
    }
   ],
   "source": [
    "### 데이터 불러오기\n",
    "import pandas as pd\n",
    "\n",
    "X_train = pd.read_csv('data/기출복원/04회/bodyPerfor_train.csv')\n",
    "X_test = pd.read_csv('data/기출복원/04회/bodyPerfor_test.csv')\n",
    "\n",
    "y_train = X_train.pop('class').copy()\n",
    "\n",
    "### EDA\n",
    "\n",
    "# 불필요 변수 'id'\n",
    "# 결측치 X\n",
    "# 구간화 'age'\n",
    "# 범주형 'gender'\n",
    "# \n",
    "\n",
    "### 데이터 전처리\n",
    "# 1) 불필요 변수 제거\n",
    "X_train = X_train.drop('id', axis = 1)\n",
    "X_test = X_test.drop('id', axis = 1)\n",
    "\n",
    "# 2) 결측치 제거\n",
    "# 3) 범주형 변수 처리\n",
    "# 4) 수치형 변수 처리\n",
    "X_train['age_gp'] = pd.cut(X_train['age'], bins = list(range(0,100,10))).astype('object')\n",
    "X_test['age_gp'] = pd.cut(X_test['age'], bins = list(range(0,100,10))).astype('object')\n",
    "\n",
    "X_train = X_train.drop('age', axis = 1)\n",
    "X_test = X_test.drop('age', axis = 1)\n",
    "\n",
    "# 5) 데이터 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                  test_size = 0.3,\n",
    "                                                  stratify = y_train,\n",
    "                                                  random_state = 2022)\n",
    "\n",
    "# 6) 인코딩\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "X_train_obj = X_train.select_dtypes('object').copy()\n",
    "X_val_obj = X_val.select_dtypes('object').copy()\n",
    "X_test_obj = X_test.select_dtypes('object').copy()\n",
    "\n",
    "ohe = OneHotEncoder(sparse = False).fit(X_train_obj)\n",
    "\n",
    "X_train_oh = ohe.transform(X_train_obj)\n",
    "X_val_oh = ohe.transform(X_val_obj)\n",
    "X_test_oh = ohe.transform(X_test_obj)\n",
    "\n",
    "# 7) 스케일링\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train_num = X_train.select_dtypes(exclude = 'object').copy()\n",
    "X_val_num = X_val.select_dtypes(exclude = 'object').copy()\n",
    "X_test_num = X_test.select_dtypes(exclude = 'object').copy()\n",
    "\n",
    "scale = StandardScaler().fit(X_train_num)\n",
    "\n",
    "X_train_scale = scale.transform(X_train_num)\n",
    "X_val_scale = scale.transform(X_val_num)\n",
    "X_test_scale = scale.transform(X_test_num)\n",
    "\n",
    "# 8) 데이터셋 준비\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.concatenate([X_train_oh, X_train_scale], axis = 1)\n",
    "X_val = np.concatenate([X_val_oh, X_val_scale], axis = 1)\n",
    "X_test = np.concatenate([X_test_oh, X_test_scale], axis = 1)\n",
    "\n",
    "y_train = y_train.values.ravel()\n",
    "y_val = y_val.values.ravel()\n",
    "\n",
    "### 모델 학습\n",
    "# 학습\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 500,\n",
    "                            max_depth = 5,\n",
    "                            min_samples_leaf = 10,\n",
    "                            random_state = 2022)\n",
    "model_rf = rf.fit(X_train, y_train)\n",
    "\n",
    "xgb = XGBClassifier(n_estimators = 300,\n",
    "                    nthread = 5,\n",
    "                    min_child_weight = 10,\n",
    "                    gamma = 0.2,\n",
    "                    objective = 'multi:softmax',\n",
    "                    use_label_encoder = False,\n",
    "                    random_state = 2022)\n",
    "model_xgb = xgb.fit(X_train, y_train, eval_metric = 'mlogloss')\n",
    "\n",
    "lgb = LGBMClassifier(n_estimators = 300,\n",
    "                     n_jobs = 5,\n",
    "                     min_child_weight = 10,\n",
    "                     learning_rate = 0.1,\n",
    "                     objective = 'multiclass',\n",
    "                     random_state = 2022)\n",
    "model_lgb = lgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dfe021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
